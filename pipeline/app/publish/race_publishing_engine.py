"""
Race Publishing Engine for SmarterVote Pipeline

This module handles the final publication of race analysis data to multiple storage systems.
It transforms arbitrated consensus data into standardized RaceJSON format and publishes
to local storage, cloud storage, databases, and notification systems.

Key responsibilities:
- Transform arbitrated data into structured RaceJSON format
- Validate and enrich race data with metadata
- Publish to multiple storage backends (local, cloud, database)
- Send completion notifications and webhook callbacks
- Maintain publication audit trails and versioning
"""

import asyncio
import json
import logging
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from ..schema import ConfidenceLevel, ProcessingStatus, RaceJSON

logger = logging.getLogger(__name__)


class PublicationTarget(str, Enum):
    """Available publication targets for race data."""

    LOCAL_FILE = "local_file"
    CLOUD_STORAGE = "cloud_storage"
    DATABASE = "database"
    WEBHOOK = "webhook"
    PUBSUB = "pubsub"
    API_ENDPOINT = "api_endpoint"


@dataclass
class PublicationResult:
    """Result of a publication operation."""

    target: PublicationTarget
    success: bool
    timestamp: datetime
    message: str
    metadata: Dict[str, Any] = None


@dataclass
class PublicationConfig:
    """Configuration for publication operations."""

    output_directory: Path
    enable_cloud_storage: bool = True
    enable_database: bool = True
    enable_webhooks: bool = True
    enable_notifications: bool = True
    version_control: bool = True
    compression: bool = False
    encryption: bool = False
    retention_days: int = 365


class RacePublishingEngine:
    """
    Advanced publishing engine for SmarterVote race analysis data.

    This engine handles the transformation of arbitrated consensus data into
    standardized RaceJSON format and publishes it across multiple systems.
    """

    def __init__(self, config: Optional[PublicationConfig] = None):
        """
        Initialize the publishing engine.

        Args:
            config: Publication configuration settings
        """
        self.config = config or PublicationConfig(output_directory=Path("data/published"))

        # Ensure output directory exists
        self.config.output_directory.mkdir(parents=True, exist_ok=True)

        # Publication tracking
        self.publication_history: List[PublicationResult] = []
        self.active_publications: Dict[str, asyncio.Task] = {}

        # Validation and transformation settings
        self.validation_rules = self._initialize_validation_rules()
        self.transformation_pipeline = self._initialize_transformation_pipeline()

        logger.info(f"Publishing engine initialized with output directory: {self.config.output_directory}")

    async def create_race_json(self, race_id: str, arbitrated_data: Dict[str, Any]) -> RaceJSON:
        """
        Transform arbitrated data into standardized RaceJSON format.

        Args:
            race_id: Unique identifier for the race
            arbitrated_data: Consensus data from arbitration engine

        Returns:
            Complete RaceJSON object ready for publication

        TODO: Implement comprehensive data transformation pipeline:
        - Extract candidate information from arbitrated summaries
        - Parse office, jurisdiction, and election metadata
        - Map confidence levels to data quality indicators
        - Generate structured issue positions and policy stances
        - Create citation and source reference mappings
        - Validate data completeness and consistency
        - Apply data enrichment from external sources
        - Generate publication metadata and versioning
        """
        logger.info(f"Creating RaceJSON for race {race_id}")

        try:
            # Validate input data
            await self._validate_arbitrated_data(arbitrated_data)

            # Extract base race information
            race_info = await self._extract_race_metadata(race_id, arbitrated_data)

            # Process candidate data
            candidates = await self._extract_candidates(arbitrated_data)

            # Generate publication metadata
            metadata = await self._generate_publication_metadata(race_id, arbitrated_data)

            # Create RaceJSON object
            race = RaceJSON(
                id=race_id,
                election_date=race_info.get("election_date", datetime(2024, 11, 5)),
                candidates=candidates,
                updated_utc=datetime.now(timezone.utc),
                generator=metadata.get("generators", ["gpt-4o", "claude-3.5", "grok-4"]),
                title=race_info.get("title", f"Electoral Race {race_id}"),
                office=race_info.get("office", "Unknown Office"),
                jurisdiction=race_info.get("jurisdiction", "Unknown Jurisdiction"),
            )

            # Apply validation rules
            await self._validate_race_json(race)

            logger.info(f"Successfully created RaceJSON for race {race_id}")
            return race

        except Exception as e:
            logger.error(f"Failed to create RaceJSON for race {race_id}: {e}")
            raise

    async def publish_race(self, race: RaceJSON, targets: Optional[List[PublicationTarget]] = None) -> List[PublicationResult]:
        """
        Publish race data to specified targets.

        Args:
            race: Complete race object to publish
            targets: List of publication targets (all if None)

        Returns:
            List of publication results for each target

        TODO: Implement parallel publication to multiple targets:
        - Local file system with versioning and backup
        - Cloud storage (GCS, S3, Azure) with CDN integration
        - Database systems (PostgreSQL, MongoDB, BigQuery)
        - Real-time notification systems (Pub/Sub, WebSockets)
        - Webhook endpoints for external system integration
        - API endpoints for immediate data availability
        - Content delivery networks for public access
        - Search index updates for discovery
        """
        if targets is None:
            # Auto-detect environment and choose appropriate targets
            targets = self._get_environment_specific_targets()

        logger.info(f"Publishing race {race.id} to {len(targets)} targets: {[t.value for t in targets]}")

        results = []
        publication_tasks = []

        # Create publication tasks for each target
        for target in targets:
            task = asyncio.create_task(self._publish_to_target(race, target))
            publication_tasks.append(task)
            self.active_publications[f"{race.id}_{target.value}"] = task

        # Execute all publications in parallel
        try:
            task_results = await asyncio.gather(*publication_tasks, return_exceptions=True)

            for i, result in enumerate(task_results):
                target = targets[i]

                if isinstance(result, Exception):
                    pub_result = PublicationResult(
                        target=target,
                        success=False,
                        timestamp=datetime.now(timezone.utc),
                        message=f"Publication failed: {str(result)}",
                    )
                else:
                    pub_result = result

                results.append(pub_result)
                self.publication_history.append(pub_result)

                # Clean up task tracking
                task_key = f"{race.id}_{target.value}"
                if task_key in self.active_publications:
                    del self.active_publications[task_key]

        except Exception as e:
            logger.error(f"Critical error during race publication: {e}")
            raise

        # Log publication summary
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]

        logger.info(f"Race {race.id} publication complete: {len(successful)} successful, {len(failed)} failed")

        if failed:
            logger.warning(f"Failed publications for race {race.id}: {[f.target.value for f in failed]}")

        return results

    async def _publish_to_target(self, race: RaceJSON, target: PublicationTarget) -> PublicationResult:
        """
        Publish race data to a specific target.

        Args:
            race: Race data to publish
            target: Publication target

        Returns:
            Publication result with success status and metadata
        """
        start_time = datetime.now(timezone.utc)

        try:
            if target == PublicationTarget.LOCAL_FILE:
                await self._publish_to_local_file(race)
            elif target == PublicationTarget.CLOUD_STORAGE:
                await self._publish_to_cloud_storage(race)
            elif target == PublicationTarget.DATABASE:
                await self._publish_to_database(race)
            elif target == PublicationTarget.WEBHOOK:
                await self._publish_to_webhooks(race)
            elif target == PublicationTarget.PUBSUB:
                await self._publish_to_pubsub(race)
            elif target == PublicationTarget.API_ENDPOINT:
                await self._publish_to_api_endpoint(race)
            else:
                raise ValueError(f"Unknown publication target: {target}")

            return PublicationResult(
                target=target,
                success=True,
                timestamp=start_time,
                message=f"Successfully published to {target.value}",
                metadata={"duration_ms": (datetime.now(timezone.utc) - start_time).total_seconds() * 1000},
            )

        except Exception as e:
            logger.error(f"Failed to publish race {race.id} to {target.value}: {e}")
            return PublicationResult(
                target=target,
                success=False,
                timestamp=start_time,
                message=f"Publication failed: {str(e)}",
                metadata={"error_type": type(e).__name__},
            )

    async def _publish_to_local_file(self, race: RaceJSON) -> None:
        """
        Publish race data to local file system.

        TODO: Implement advanced local file publishing:
        - JSON pretty-printing with consistent formatting
        - File versioning with timestamp suffixes
        - Atomic write operations to prevent corruption
        - Backup creation before overwriting existing files
        - Directory organization by date/jurisdiction
        - Compression for large race files
        - Checksum generation for integrity verification
        - Symbolic link creation for latest version
        """
        try:
            # Convert to JSON with proper formatting
            race_json = race.model_dump(mode="json")

            # Generate output filename with versioning
            output_file = self.config.output_directory / f"{race.id}.json"

            # Create backup if file exists
            if output_file.exists() and self.config.version_control:
                backup_file = self.config.output_directory / f"{race.id}.json.backup"
                output_file.rename(backup_file)

            # Write race data atomically
            temp_file = output_file.with_suffix(".tmp")
            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(race_json, f, indent=2, ensure_ascii=False, default=str)

            # Atomic rename to final filename
            temp_file.rename(output_file)

            logger.debug(f"Successfully published race {race.id} to local file: {output_file}")

        except Exception as e:
            logger.error(f"Failed to publish race {race.id} to local file: {e}")
            raise

    async def _publish_to_cloud_storage(self, race: RaceJSON) -> None:
        """
        Publish race data to Google Cloud Storage.

        Implements cloud storage integration with proper error handling,
        metadata, and content type configuration.
        """
        try:
            import os

            from google.cloud import storage
            from google.cloud.exceptions import GoogleCloudError

            # Get configuration from environment
            project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
            bucket_name = os.getenv("GCS_BUCKET_NAME")

            if not project_id or not bucket_name:
                raise ValueError("Missing required GCP configuration: GOOGLE_CLOUD_PROJECT and GCS_BUCKET_NAME")

            # Initialize client
            client = storage.Client(project=project_id)
            bucket = client.bucket(bucket_name)

            # Generate blob name with versioning
            blob_name = f"races/{race.id}.json"
            blob = bucket.blob(blob_name)

            # Convert race to JSON
            race_json = race.model_dump(mode="json")
            race_data = json.dumps(race_json, indent=2, ensure_ascii=False, default=str)

            # Set metadata
            metadata = {
                "race_id": race.id,
                "updated_utc": race.updated_utc.isoformat(),
                "pipeline_version": "1.0.0",
                "content_type": "application/json",
                "encoding": "utf-8",
            }

            # Upload with proper content type and metadata
            blob.upload_from_string(race_data, content_type="application/json; charset=utf-8")

            # Update blob metadata
            blob.metadata = metadata
            blob.patch()

            # Make publicly readable if configured
            if os.getenv("GCS_PUBLIC_READ", "false").lower() == "true":
                blob.make_public()

            logger.info(f"Successfully published race {race.id} to GCS bucket {bucket_name}")

        except ImportError:
            logger.error("Google Cloud Storage client not available. Install with: pip install google-cloud-storage")
            raise
        except GoogleCloudError as e:
            logger.error(f"GCS error publishing race {race.id}: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to publish race {race.id} to cloud storage: {e}")
            raise

    async def _publish_to_database(self, race: RaceJSON) -> None:
        """
        Publish race data to database systems.

        Implements database publication with proper transaction handling,
        conflict resolution, and data normalization.
        """
        try:
            import os

            import asyncpg
            from asyncpg.exceptions import PostgresError

            # Get database configuration
            database_url = os.getenv("DATABASE_URL")

            if not database_url:
                logger.warning("DATABASE_URL not configured, skipping database publication")
                return

            # Connect to database
            conn = await asyncpg.connect(database_url)

            try:
                # Start transaction
                async with conn.transaction():
                    # Convert race to dict for database storage
                    race_data = race.model_dump(mode="json")

                    # Upsert race record
                    await conn.execute(
                        """
                        INSERT INTO races (id, data, updated_utc, title, office, jurisdiction, election_date)
                        VALUES ($1, $2, $3, $4, $5, $6, $7)
                        ON CONFLICT (id)
                        DO UPDATE SET
                            data = EXCLUDED.data,
                            updated_utc = EXCLUDED.updated_utc,
                            title = EXCLUDED.title,
                            office = EXCLUDED.office,
                            jurisdiction = EXCLUDED.jurisdiction,
                            election_date = EXCLUDED.election_date
                    """,
                        race.id,
                        json.dumps(race_data, default=str),
                        race.updated_utc,
                        race.title,
                        race.office,
                        race.jurisdiction,
                        race.election_date,
                    )

                    # Insert/update candidate records for easier querying
                    for candidate in race.candidates:
                        await conn.execute(
                            """
                            INSERT INTO candidates (race_id, name, party, incumbent, summary)
                            VALUES ($1, $2, $3, $4, $5)
                            ON CONFLICT (race_id, name)
                            DO UPDATE SET
                                party = EXCLUDED.party,
                                incumbent = EXCLUDED.incumbent,
                                summary = EXCLUDED.summary
                        """,
                            race.id,
                            candidate.name,
                            candidate.party,
                            candidate.incumbent,
                            candidate.summary,
                        )

                logger.info(f"Successfully published race {race.id} to database")

            finally:
                await conn.close()

        except ImportError:
            logger.error("AsyncPG not available. Install with: pip install asyncpg")
            raise
        except PostgresError as e:
            logger.error(f"Database error publishing race {race.id}: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to publish race {race.id} to database: {e}")
            raise

    async def _publish_to_webhooks(self, race: RaceJSON) -> None:
        """
        Send race data to configured webhook endpoints.

        Implements webhook notification system with proper security,
        retry logic, and response handling.
        """
        try:
            import hashlib
            import hmac
            import os

            import aiohttp
            from aiohttp import ClientError, ClientTimeout

            # Get webhook configuration
            webhook_urls = os.getenv("WEBHOOK_URLS", "").split(",")
            webhook_secret = os.getenv("WEBHOOK_SECRET")
            webhook_timeout = int(os.getenv("WEBHOOK_TIMEOUT", "30"))

            # Filter out empty URLs
            webhook_urls = [url.strip() for url in webhook_urls if url.strip()]

            if not webhook_urls:
                logger.debug(f"No webhook URLs configured for race {race.id}")
                return

            # Prepare payload
            payload = {
                "event": "race_published",
                "race_id": race.id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "data": race.model_dump(mode="json"),
            }

            payload_json = json.dumps(payload, default=str, separators=(",", ":"))

            # Generate signature if secret is configured
            signature = None
            if webhook_secret:
                signature = hmac.new(webhook_secret.encode("utf-8"), payload_json.encode("utf-8"), hashlib.sha256).hexdigest()

            # Send to all configured webhooks
            timeout = ClientTimeout(total=webhook_timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                for url in webhook_urls:
                    try:
                        headers = {
                            "Content-Type": "application/json",
                            "User-Agent": "SmarterVote-Publisher/1.0",
                            "X-SmarterVote-Event": "race_published",
                            "X-SmarterVote-Race-ID": race.id,
                        }

                        if signature:
                            headers["X-SmarterVote-Signature"] = f"sha256={signature}"

                        async with session.post(url, data=payload_json, headers=headers) as response:
                            if response.status >= 200 and response.status < 300:
                                logger.info(f"Successfully sent webhook for race {race.id} to {url}")
                            else:
                                response_text = await response.text()
                                logger.warning(
                                    f"Webhook failed for race {race.id} to {url}: {response.status} - {response_text}"
                                )

                    except ClientError as e:
                        logger.error(f"Network error sending webhook for race {race.id} to {url}: {e}")
                    except Exception as e:
                        logger.error(f"Error sending webhook for race {race.id} to {url}: {e}")

            logger.info(f"Completed webhook notifications for race {race.id}")

        except ImportError:
            logger.error("aiohttp not available. Install with: pip install aiohttp")
            raise
        except Exception as e:
            logger.error(f"Failed to send webhooks for race {race.id}: {e}")
            raise

    async def _publish_to_pubsub(self, race: RaceJSON) -> None:
        """
        Publish race data to Google Cloud Pub/Sub.

        Implements Pub/Sub messaging with proper error handling,
        message ordering, and metadata.
        """
        try:
            import os

            from google.cloud import pubsub_v1
            from google.cloud.exceptions import GoogleCloudError

            # Get configuration
            project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
            topic_name = os.getenv("PUBSUB_RACE_TOPIC", "race-published")

            if not project_id:
                raise ValueError("Missing required GCP configuration: GOOGLE_CLOUD_PROJECT")

            # Initialize publisher
            publisher = pubsub_v1.PublisherClient()
            topic_path = publisher.topic_path(project_id, topic_name)

            # Prepare message
            message_data = {
                "event": "race_published",
                "race_id": race.id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "office": race.office,
                "jurisdiction": race.jurisdiction,
                "election_date": race.election_date.isoformat() if race.election_date else None,
                "candidate_count": len(race.candidates),
                "updated_utc": race.updated_utc.isoformat(),
            }

            message_json = json.dumps(message_data, default=str)

            # Message attributes for filtering
            attributes = {
                "event_type": "race_published",
                "race_id": race.id,
                "office": race.office or "unknown",
                "jurisdiction": race.jurisdiction or "unknown",
            }

            # Publish message with ordering key for consistent processing
            future = publisher.publish(topic_path, message_json.encode("utf-8"), ordering_key=race.id, **attributes)

            # Wait for publish to complete
            message_id = future.result(timeout=30)

            logger.info(f"Successfully published race {race.id} to Pub/Sub topic {topic_name}, message ID: {message_id}")

        except ImportError:
            logger.error("Google Cloud Pub/Sub client not available. Install with: pip install google-cloud-pubsub")
            raise
        except GoogleCloudError as e:
            logger.error(f"Pub/Sub error publishing race {race.id}: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to publish race {race.id} to Pub/Sub: {e}")
            raise

    async def _publish_to_api_endpoint(self, race: RaceJSON) -> None:
        """
        Publish race data to API endpoints for immediate availability.

        TODO: Implement API endpoint integration:
        - RESTful API POST/PUT requests to race endpoints
        - GraphQL mutation execution
        - API authentication and authorization
        - Request rate limiting and throttling
        - API response validation and error handling
        - Circuit breaker pattern for reliability
        - API versioning and compatibility
        - Real-time WebSocket updates
        """
        # Placeholder implementation
        logger.debug(f"Would publish race {race.id} to API endpoints")
        await asyncio.sleep(0.1)  # Simulate API call

    async def _validate_arbitrated_data(self, arbitrated_data: Dict[str, Any]) -> None:
        """
        Validate arbitrated data before transformation.

        TODO: Implement comprehensive data validation:
        - Schema validation against expected data structure
        - Consensus quality threshold checks
        - Required field presence validation
        - Data type and format validation
        - Confidence level validation
        - Source reference validation
        - Content length and quality checks
        - Cross-reference consistency validation
        """
        if not arbitrated_data:
            raise ValueError("Arbitrated data cannot be empty")

        # Basic validation placeholder
        required_fields = ["consensus_data", "overall_confidence"]
        for field in required_fields:
            if field not in arbitrated_data:
                logger.warning(f"Missing expected field in arbitrated data: {field}")

    async def _extract_race_metadata(self, race_id: str, arbitrated_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract race metadata from arbitrated data.

        Implements sophisticated metadata extraction from consensus data
        including parsing dates, offices, and jurisdictions.
        """
        metadata = {
            "title": f"Electoral Race {race_id}",
            "office": "Unknown Office",
            "jurisdiction": "Unknown Jurisdiction",
            "election_date": datetime(2024, 11, 5),
        }

        try:
            # Extract from arbitrated data structure
            consensus_data = arbitrated_data.get("consensus_data", {})

            # Try to extract race title
            if "race_title" in consensus_data:
                metadata["title"] = consensus_data["race_title"]
            elif "title" in consensus_data:
                metadata["title"] = consensus_data["title"]

            # Extract office information
            if "office" in consensus_data:
                metadata["office"] = consensus_data["office"]
            elif "position" in consensus_data:
                metadata["office"] = consensus_data["position"]

            # Extract jurisdiction
            if "jurisdiction" in consensus_data:
                metadata["jurisdiction"] = consensus_data["jurisdiction"]
            elif "district" in consensus_data:
                metadata["jurisdiction"] = consensus_data["district"]
            elif "state" in consensus_data:
                metadata["jurisdiction"] = consensus_data["state"]

            # Parse election date
            if "election_date" in consensus_data:
                date_str = consensus_data["election_date"]
                if isinstance(date_str, str):
                    # Try to parse various date formats
                    for fmt in ["%Y-%m-%d", "%m/%d/%Y", "%B %d, %Y"]:
                        try:
                            metadata["election_date"] = datetime.strptime(date_str, fmt)
                            break
                        except ValueError:
                            continue
                elif isinstance(date_str, datetime):
                    metadata["election_date"] = date_str

            # Infer from race_id if possible
            race_parts = race_id.split("-")
            if len(race_parts) >= 2:
                # Extract state/jurisdiction from race ID
                potential_state = race_parts[0].upper()
                if len(potential_state) == 2:  # State abbreviation
                    if metadata["jurisdiction"] == "Unknown Jurisdiction":
                        metadata["jurisdiction"] = potential_state

                # Extract office type from race ID
                if "senate" in race_id.lower():
                    if metadata["office"] == "Unknown Office":
                        metadata["office"] = "U.S. Senate"
                elif "house" in race_id.lower():
                    if metadata["office"] == "Unknown Office":
                        metadata["office"] = "U.S. House of Representatives"
                elif "governor" in race_id.lower():
                    if metadata["office"] == "Unknown Office":
                        metadata["office"] = "Governor"

                # Extract year from race ID
                if race_parts[-1].isdigit() and len(race_parts[-1]) == 4:
                    year = int(race_parts[-1])
                    if 2020 <= year <= 2030:  # Reasonable range
                        # Assume November election
                        metadata["election_date"] = datetime(year, 11, 5)

        except Exception as e:
            logger.warning(f"Error extracting race metadata for {race_id}: {e}")

        logger.debug(f"Extracted metadata for race {race_id}: {metadata}")
        return metadata

    async def _extract_candidates(self, arbitrated_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Extract candidate information from arbitrated data.

        Implements comprehensive candidate extraction from consensus summaries
        including policy positions, biographical data, and issue stances.
        """
        candidates = []

        try:
            consensus_data = arbitrated_data.get("consensus_data", {})

            # Look for candidate data in various possible structures
            candidate_summaries = []

            # Check for direct candidate summaries
            if "candidate_summaries" in consensus_data:
                candidate_summaries = consensus_data["candidate_summaries"]
            elif "candidates" in consensus_data:
                candidate_summaries = consensus_data["candidates"]

            # If no direct candidate data, try to extract from arbitrated summaries
            if not candidate_summaries and "arbitrated_summaries" in arbitrated_data:
                summaries = arbitrated_data["arbitrated_summaries"]
                for summary_data in summaries:
                    if summary_data.get("query_type") == "candidate_summary":
                        candidate_name = summary_data.get("candidate_name")
                        if candidate_name:
                            candidate_summaries.append(
                                {
                                    "name": candidate_name,
                                    "summary": summary_data.get("content", ""),
                                    "confidence": summary_data.get("confidence", "unknown"),
                                }
                            )

            # Process each candidate
            for candidate_data in candidate_summaries:
                if not isinstance(candidate_data, dict):
                    continue

                candidate = {
                    "name": candidate_data.get("name", "Unknown Candidate"),
                    "party": candidate_data.get("party"),
                    "incumbent": candidate_data.get("incumbent", False),
                    "summary": candidate_data.get("summary", "No summary available"),
                    "issues": {},
                    "top_donors": [],
                    "website": candidate_data.get("website"),
                    "social_media": candidate_data.get("social_media", {}),
                }

                # Extract issue stances from arbitrated data
                if "arbitrated_summaries" in arbitrated_data:
                    for summary_data in arbitrated_data["arbitrated_summaries"]:
                        if (
                            summary_data.get("query_type") == "issue_stance"
                            and summary_data.get("candidate_name") == candidate["name"]
                        ):
                            issue = summary_data.get("issue")
                            if issue:
                                stance = {
                                    "issue": issue,
                                    "stance": summary_data.get("content", "Position unclear"),
                                    "confidence": summary_data.get("confidence", "unknown"),
                                    "sources": [],
                                }
                                candidate["issues"][issue] = stance

                # Clean up and validate candidate data
                if len(candidate["summary"]) < 10:
                    candidate["summary"] = f"Information about {candidate['name']} is being processed."

                # Ensure required fields
                if not candidate["name"] or candidate["name"] == "Unknown Candidate":
                    continue  # Skip candidates without names

                candidates.append(candidate)

            # If no candidates were extracted, create a placeholder
            if not candidates:
                logger.warning("No candidates extracted from arbitrated data, creating placeholder")
                candidates.append(
                    {
                        "name": "Candidate Information Pending",
                        "party": None,
                        "incumbent": False,
                        "summary": "Candidate information is being processed and will be available soon. This is a longer placeholder to meet content requirements for publication validation.",
                        "issues": {},
                        "top_donors": [],
                        "website": None,
                        "social_media": {},
                    }
                )

        except Exception as e:
            logger.error(f"Error extracting candidates from arbitrated data: {e}")
            # Return minimal candidate structure to prevent pipeline failure
            candidates = [
                {
                    "name": "Data Processing Error",
                    "party": None,
                    "incumbent": False,
                    "summary": "There was an error processing candidate information. Please check the data sources and try again with updated inputs.",
                    "issues": {},
                    "top_donors": [],
                    "website": None,
                    "social_media": {},
                }
            ]

        logger.debug(f"Extracted {len(candidates)} candidates from arbitrated data")
        return candidates

    async def _generate_publication_metadata(self, race_id: str, arbitrated_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate metadata for the publication process.

        TODO: Implement comprehensive metadata generation:
        - Processing pipeline version and configuration
        - Source data fingerprints and checksums
        - AI model versions and parameters used
        - Confidence metrics and quality scores
        - Processing timestamps and duration
        - Data lineage and transformation history
        - Validation results and warnings
        - Publication target configurations
        """
        return {
            "generators": ["gpt-4o", "claude-3.5", "grok-4"],
            "processing_version": "1.0.0",
            "publication_timestamp": datetime.now(timezone.utc).isoformat(),
        }

    async def _validate_race_json(self, race: RaceJSON) -> None:
        """
        Validate RaceJSON object before publication.

        Implements comprehensive validation including business rules,
        data completeness, and publication readiness checks.
        """
        validation_errors = []

        try:
            # Basic Pydantic model validation
            race.model_validate(race.model_dump())
        except Exception as e:
            validation_errors.append(f"Pydantic validation failed: {e}")

        # Business rule validations
        if not race.id or not race.id.strip():
            validation_errors.append("Race ID is required and cannot be empty")

        if not race.title or not race.title.strip():
            validation_errors.append("Race title is required and cannot be empty")

        if not race.candidates:
            validation_errors.append("Race must have at least one candidate")

        # Validate candidates
        candidate_names = set()
        for i, candidate in enumerate(race.candidates):
            if not candidate.name or not candidate.name.strip():
                validation_errors.append(f"Candidate {i} name is required")

            if candidate.name in candidate_names:
                validation_errors.append(f"Duplicate candidate name: {candidate.name}")
            candidate_names.add(candidate.name)

            if not candidate.summary or len(candidate.summary.strip()) < 10:
                validation_errors.append(f"Candidate {candidate.name} summary is too short (minimum 10 characters)")

        # Date validations
        if race.election_date and race.election_date < datetime(1900, 1, 1):
            validation_errors.append("Election date cannot be before 1900")

        if race.updated_utc and race.updated_utc > datetime.now(timezone.utc):
            validation_errors.append("Updated timestamp cannot be in the future")

        # Content quality checks
        min_confidence = self.validation_rules.get("min_confidence", ConfidenceLevel.MEDIUM)

        # Check if we have enough high-quality content
        total_content_length = sum(len(candidate.summary) for candidate in race.candidates)
        min_content_length = self.validation_rules.get("min_content_length", 100)

        if total_content_length < min_content_length:
            validation_errors.append(f"Total content length ({total_content_length}) below minimum ({min_content_length})")

        # Validate required fields are present
        required_fields = self.validation_rules.get("required_fields", [])
        race_dict = race.model_dump()

        for field in required_fields:
            if field not in race_dict or not race_dict[field]:
                validation_errors.append(f"Required field missing or empty: {field}")

        # Check for suspicious or invalid data
        if race.office and len(race.office) > 200:
            validation_errors.append("Office name is suspiciously long (>200 characters)")

        if race.jurisdiction and len(race.jurisdiction) > 200:
            validation_errors.append("Jurisdiction name is suspiciously long (>200 characters)")

        # If there are validation errors, raise exception
        if validation_errors:
            error_message = f"RaceJSON validation failed for {race.id}: " + "; ".join(validation_errors)
            logger.error(error_message)
            raise ValueError(error_message)

        logger.debug(f"RaceJSON validation passed for race {race.id}")

    def _initialize_validation_rules(self) -> Dict[str, Any]:
        """
        Initialize data validation rules and thresholds.

        TODO: Define comprehensive validation rules:
        - Minimum content length requirements
        - Required confidence levels for publication
        - Mandatory field validation rules
        - Data quality score thresholds
        - Source reference requirements
        - Candidate information completeness rules
        - Date and time validation constraints
        - Geographic and jurisdictional validation
        """
        return {
            "min_confidence": ConfidenceLevel.MEDIUM,
            "required_fields": ["id", "title", "office"],
            "min_content_length": 100,
        }

    def _initialize_transformation_pipeline(self) -> List[str]:
        """
        Initialize data transformation pipeline stages.

        TODO: Define transformation pipeline:
        - Data cleaning and normalization
        - Entity extraction and linking
        - Content summarization and structuring
        - Metadata enrichment and augmentation
        - Quality scoring and validation
        - Format conversion and standardization
        - Reference resolution and validation
        - Final packaging and optimization
        """
        return [
            "data_cleaning",
            "entity_extraction",
            "content_structuring",
            "metadata_enrichment",
            "quality_validation",
            "format_standardization",
        ]

    def get_publication_history(self, race_id: Optional[str] = None) -> List[PublicationResult]:
        """
        Get publication history for all races or a specific race.

        Args:
            race_id: Optional race ID to filter history

        Returns:
            List of publication results
        """
        if race_id:
            return [r for r in self.publication_history if race_id in str(r.metadata)]
        return self.publication_history.copy()

    def get_published_races(self) -> List[str]:
        """
        Get list of successfully published race IDs.

        Returns:
            List of race IDs that have been published

        TODO: Implement comprehensive race listing:
        - Scan multiple storage backends for published races
        - Aggregate publication status across targets
        - Include publication timestamps and versions
        - Filter by publication status and targets
        - Support pagination for large race counts
        - Include metadata and quality scores
        """
        published_files = list(self.config.output_directory.glob("*.json"))
        return [f.stem for f in published_files]

    def get_race_data(self, race_id: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve published race data by ID.

        Args:
            race_id: Race identifier

        Returns:
            Race data dictionary or None if not found

        TODO: Implement multi-source race data retrieval:
        - Check local file system first
        - Fall back to cloud storage if local not available
        - Query database for most recent version
        - Handle version conflicts and merging
        - Cache frequently accessed race data
        - Support partial data retrieval for large races
        """
        race_file = self.config.output_directory / f"{race_id}.json"

        if race_file.exists():
            try:
                with open(race_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load race data for {race_id}: {e}")
                return None

        return None

    async def cleanup_old_publications(self, retention_days: Optional[int] = None) -> int:
        """
        Clean up old publication files based on retention policy.

        Args:
            retention_days: Number of days to retain files (uses config default if None)

        Returns:
            Number of files cleaned up

        TODO: Implement comprehensive cleanup:
        - Clean up local files older than retention period
        - Archive old files to long-term storage
        - Clean up cloud storage objects with lifecycle policies
        - Remove old database records with archival
        - Clean up publication history and logs
        - Generate cleanup reports and notifications
        """
        days = retention_days or self.config.retention_days
        cutoff_date = datetime.now() - timedelta(days=days)

        cleanup_count = 0

        for file_path in self.config.output_directory.glob("*.json"):
            if file_path.stat().st_mtime < cutoff_date.timestamp():
                try:
                    file_path.unlink()
                    cleanup_count += 1
                    logger.debug(f"Cleaned up old publication file: {file_path}")
                except Exception as e:
                    logger.warning(f"Failed to clean up file {file_path}: {e}")

        logger.info(f"Cleaned up {cleanup_count} old publication files")
        return cleanup_count

    def _get_environment_specific_targets(self) -> List[PublicationTarget]:
        """
        Detect environment and return appropriate publication targets.
        
        Returns:
            List of publication targets based on detected environment
            
        Environment Detection Logic:
        - Local: Publish to local files only
        - Cloud: Publish to cloud storage, database, pub/sub, webhooks
        """
        import os
        
        # Check for cloud environment indicators
        cloud_indicators = [
            os.getenv("GOOGLE_CLOUD_PROJECT"),
            os.getenv("CLOUD_RUN_SERVICE"),
            os.getenv("K_SERVICE"),  # Cloud Run service name
            os.getenv("GAE_APPLICATION"),  # App Engine
            os.getenv("FUNCTION_NAME"),  # Cloud Functions
        ]
        
        is_cloud_environment = any(cloud_indicators)
        
        if is_cloud_environment:
            logger.info("ðŸŒ©ï¸  Detected cloud environment - using cloud publication targets")
            targets = [
                PublicationTarget.CLOUD_STORAGE,
                PublicationTarget.DATABASE,
                PublicationTarget.PUBSUB,
                PublicationTarget.WEBHOOK,
                PublicationTarget.LOCAL_FILE,  # Also save locally for backup
            ]
        else:
            logger.info("ðŸ’» Detected local environment - using local publication targets")
            targets = [
                PublicationTarget.LOCAL_FILE,
            ]
            
        logger.info(f"Selected publication targets: {[t.value for t in targets]}")
        return targets

    def _calculate_publication_metrics(self, results: List[PublicationResult]) -> Dict[str, Any]:
        """Calculate metrics about publication results."""
        total = len(results)
        successful = sum(1 for r in results if r.success)
        failed = total - successful

        return {
            "total_targets": total,
            "successful_publications": successful,
            "failed_publications": failed,
            "success_rate": successful / total if total > 0 else 0.0,
            "publication_targets": [r.target.value for r in results],
            "publication_time": datetime.now(timezone.utc),
        }
